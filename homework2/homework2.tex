\documentclass[twoside,10pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx,hyperref}


\begin{document}

\title{ISYE 6740 Summer 2023\\ Homework 2 \\
(100 points + 5 bonus points)}
%\author{Yao Xie}
\date{}

\maketitle

\subsection*{1. Conceptual questions [20 points].}


\begin{enumerate}

\item (5 points) Please prove the first principle component direction $v$ corresponds to the largest eigenvector of the sample covariance matrix:
\[
v = \arg\max_{w: \|w\|\leq 1} \frac 1 m \sum_{i=1}^m (w^T x^i - w^T \mu)^2.
\]
You may use the proof steps in the lecture, but please write them logically and cohesively.

\item (5 points) Based on your answer to the question above, explain how to further find the second, and the third largest principle component directions.

\item (5 points) Based on the outline given in the lecture, show that the maximum likelihood estimate (MLE) for Gaussian mean and variance parameters are given by 
\[
\hat \mu = \frac 1 m \sum_{i=1}^m x^i, \quad \hat \sigma^2 = \frac 1 m \sum_{i=1}^m (x^i - \hat \mu)^2,
\]
respectively. Please show the work for your derivations in full detail.

%\item (5 points) Explain the three key ideas in ISOMAP (for manifold learning and non-linear dimensionality reduction). 


\item (5 points) Please compare the pros and cons of KDE over histogram, and give at least one advantage and disadvantage to each.

\end{enumerate}


\clearpage

\subsection*{2. PCA: Food consumption in European countries [15 points].}

The data \textsf{food-consumption.csv} contains 16 countries in Europe and their consumption for 20 food items, such as tea, jam, coffee, yogurt, and others. We will perform principal component analysis to explore the data. In this question, please implement PCA by writing your own code (you can use any basic packages, such as numerical linear algebra, reading data, in your file).


\vspace{.1in}
First, we will perform PCA analysis on the data by treating each country's food consumption as their ``feature'' vectors. In other words, we will find weight vectors to combine 20 food-item consumptions for each country.  
 
\begin{enumerate}

\item[(a)] (10 points) For this problem of performing PCA on countries by treating each country's food consumption as their ``feature'' vectors, explain how the data matrix is set-up in this case (e.g., the columns and the rows of the matrix correspond to what). Now extract the first two principal components for each data point (thus, this means we will represent each data point using a two-dimensional vector). Draw a scatter plot of two-dimensional representations of the countries using their two principal components. Mark the countries on the plot (you can do this by hand if you want). Please explain any pattern you observe in the scatter plot.

\item[(b)] (5 points) Now, we will perform PCA analysis on the data by treating country consumptions as ``feature'' vectors for each food item. In other words, we will now find weight vectors to combine country consumptions for each food item to perform PCA another way. Project data to obtain their two principle components (thus, again each data point -- for each food item -- can be represented using a two-dimensional vector). Draw a scatter plot of food items. Mark the food items on the plot (you can do this by hand if you want). Please explain any pattern you observe in the scatter plot.
\end{enumerate}


\clearpage

\subsection*{3. Order of faces using ISOMAP [20 points]}

This question aims to reproduce the ISOMAP algorithm results in the original paper for ISOMAP, J.B. Tenenbaum, V. de Silva, and J.C. Langford, Science 290 (2000) 2319-2323 that we have also seen in the lecture as an exercise (isn't this exciting to go through the process of generating results for a high-impact research paper!) 


The file \textsf{isomap.mat} (or \textsf{isomap.dat}) contains 698 images, corresponding to different poses of the same face. Each image is given as a 64 $\times$ 64 luminosity map, hence represented as a vector in $\mathbb R^{4096}$. This vector is stored as a row in the file. (This is one of the datasets used in the original paper.) In this question, you are expected to implement the ISOMAP algorithm by coding it up yourself. You may find the shortest path (required by one step of the algorithm), using \url{https://scikit-learn.org/stable/modules/generated/sklearn.utils.graph\_shortest\_path.graph\_shortest_path.html}. 

Using Euclidean distance (i.e., in this case, a distance in $\mathbb R^{4096}$) to construct the $\epsilon$-ISOMAP (follow the instructions in the slides.) You will tune the $\epsilon$ parameter to achieve the most reasonable performance. Please note that this is different from $K$-ISOMAP, where each node has exactly $K$ nearest neighbors.

\begin{enumerate} 

\item[(a)] (5 points) Visualize the nearest neighbor graph (you can either show the adjacency matrix (e.g., as an image), or visualize the graph similar to the lecture slides using graph visualization packages such as Gephi (\textsf{https://gephi.org}) and illustrate a few images corresponds to nodes at different parts of the graph, e.g., mark them by hand or use software packages).
 
\item[(b)] (10 points) Implement the ISOMAP algorithm yourself to obtain a two-dimensional low-dimensional embedding. Plot the embeddings using a scatter plot, similar to the plots in lecture slides. Find a few images in the embedding space and show what these images look like and specify the face locations on the scatter plot. Comment on do you see any visual similarity among them and their arrangement, similar to what you seen in the paper?

\item[(c)] (5 points) Perform PCA (you can now use your implementation written in Question 1) on the images and project them into the top 2 principal components. Again show them on a scatter plot. Explain whether or you see a more meaningful projection using ISOMAP than PCA. 

\end{enumerate}



\clearpage

\subsection*{4. Eigenfaces and simple face recognition [20 points].}

This question is a simplified illustration of using PCA for face recognition. We will use a subset of data from the famous Yale Face dataset. 

\vspace{.1in}
\noindent
{\bf Remark:} You will have to perform downsampling of the image by a factor of 4 to turn them into a lower resolution image as a preprocessing (e.g., reduce a picture of size 16-by-16 to 4-by-4). In this question, you can implement your own code or call packages. 

First, given a set of images for each person, we generate the eigenface using these images. You will treat one picture from the same person as one data point for that person. Note that you will first vectorize each image, which was originally a matrix. Thus, the data matrix (for each person) is a matrix; each row is a vectorized picture. You will find weight vectors to combine the pictures to extract different ``eigenfaces'' that correspond to that person's pictures' first few principal components. 


\begin{enumerate}

\item[(a)] (10 points) Perform analysis on the Yale face dataset for Subject 1 and Subject 2, respectively, using all the images EXCEPT for the two pictures named \textsf{subject01-test.gif} and \textsf{subject02-test.gif}. {\bf Plot the first 6 eigenfaces for each subject.} When visualizing, please reshape the eigenvectors into proper images. Please explain can you see any patterns in the top 6 eigenfaces?

\item[(b)] (5 points) Now we will perform a simple face recognition task. 

Face recognition through PCA is proceeded as follows. Given the test image \textsf{subject01-test.gif} and \textsf{subject02-test.gif}, first downsize by a factor of 4 (as before), and vectorize each image. Take the top eigenfaces of Subject 1 and Subject 2, respectively. Then we calculate the {\it projection residual} of the 2 vectorized test images with the vectorized eigenfaces:
\[s_{ij} = \|\textsf{(test image)}_j - \textsf{(eigenface}_i)\textsf{(eigenface})_i^T \textsf{(test image)}_j\|_2^2\]

Report all four scores: $s_{ij}$, $i = 1, 2$, $j = 1, 2.$ Explain how to recognize the faces of the test images using these scores. 

\item[(c)] (5 points) Comment if your face recognition algorithm works well and discuss how you would like to improve it if possible.

\end{enumerate}


\clearpage



\subsection*{5. Density estimation: Psychological experiments. [25 points]}


In {\it Kanai, R., Feilden, T., Firth, C. and Rees, G., 2011. Political orientations are correlated with brain structure in young adults. Current biology, 21(8), pp.677-680.}, data are collected to  study whether or not the two brain regions are likely to be independent of each other and considering different types of political view \textbf{For this question; you can use third party histogram and KDE packages; no need to write your own.} The data set \textsf{n90pol.csv} contains information on 90 university students who participated in a psychological experiment designed to look for relationships between the size of different regions of the brain and political views. The variables \textsf{amygdala} and \textsf{acc} indicate the volume of two particular brain regions known to be involved in emotions and decision-making, the amygdala and the anterior cingulate cortex; more exactly, these are residuals from the predicted volume, after adjusting for height, sex, and similar body-type variables. The variable \textsf{orientation} gives the students' locations on a five-point scale from 1 (very conservative) to 5 (very liberal).  Note that in the dataset, we only have observations for orientation from 2 to 5. 

Recall in this case, the kernel density estimator (KDE) for a density is given by
 \[
 p(x) = \frac 1 m \sum_{i=1}^m \frac 1 h
 K\left(
 \frac{x^i - x}{h}
 \right),
 \]
where $x^i$ are two-dimensional vectors, $h >0$ is the kernel bandwidth, based on the criterion we discussed in lecture. 
For one-dimensional KDE,  use a one-dimensional Gaussian kernel
\[
K(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}.
\]
For two-dimensional KDE, use a two-dimensional Gaussian kernel: for \[x = \begin{bmatrix}x_1\\x_2\end{bmatrix}\in \mathbb R^2,\] where $x_1$ and $x_2$ are the two dimensions respectively \[K(x) = \frac{1}{2\pi} e^{-\frac{(x_1)^2 + (x_2)^2}{2}}.\] 
  
 \begin{enumerate}
 
 
 \item[(a)] (5 points) Form the 1-dimensional histogram and KDE to estimate the distributions of \textsf{amygdala} and \textsf{acc}, respectively. For this question, you can ignore the variable \textsf{orientation}. Decide on a suitable number of bins so you can see the shape of the distribution clearly. Set an appropriate kernel bandwidth $h >0$. 
 
%For example. for one-dimensional KDE, you are welcome to use a rule-of-thumb bandwidth estimator
%\[
%h \approx 1.06 \hat \sigma n^{-1/5}, 
%\]
%where $n$ is the sample size, $\hat \sigma$ is the standard error of samples; this is shown to be optimal when Gaussian kernel functions are used for univariate data.
 
 
 \item[(b)] (5 points) Form 2-dimensional histogram for the pairs of variables (\textsf{amygdala}, \textsf{acc}). Decide on a suitable number of bins so you can see the shape of the distribution clearly. 
 
 \item[(c)] (5 points) Use kernel-density-estimation (KDE) to estimate the 2-dimensional density function of (\textsf{amygdala}, \textsf{acc}) (this means for this question, you can ignore the variable \textsf{orientation}). Set an appropriate kernel bandwidth $h >0$. 

Please show the two-dimensional KDE (e.g., two-dimensional heat-map, two-dimensional contour plot, etc.)

Please explain what you have observed: is the distribution unimodal or bi-modal? Are there any outliers?

Please explain based on the results, can you infer that the two variables (\textsf{amygdala}, \textsf{acc}) are likely to be independent or not?


 \item[(d)] (5 points) We will consider the variable \textsf{orientation} and consider conditional distributions. Please plot the estimated conditional distribution of \textsf{amygdala} conditioning on political \textsf{orientation}: $p(\textsf{amygdala}|\textsf{orientation}=c)$, $c = 2, \ldots, 5$, using KDE. Set an appropriate kernel bandwidth $h >0$.  Do the same for the volume of the \textsf{acc}: plot $p(\textsf{acc}|\textsf{orientation}=c)$, $c = 2, \ldots, 5$ using KDE. (Note that the conditional distribution can be understood as fitting a distribution for the data with the same $\textsf{orientation}$. Thus you should plot 8 one-dimensional distribution functions in total for this question.) 
 
 
Now please explain based on the results, can you infer that the conditional distribution of  \textsf{amygdala} and \textsf{acc}, respectively, are different from $c = 2, \ldots, 5$? This is a type of scientific question one could infer from the data: Whether or not there is a difference between brain structure and political view. 

Now please also fill out the {\it conditional sample mean} for the two variables: %
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
& $c = 2$ & $c = 3$ & $c = 4$ & $c = 5$ \\\hline
\textsf{amygdala} & & & & \\\hline
\textsf{acc} & & & & \\\hline
\end{tabular}
\end{center}
Remark: As you can see this exercise, you can extract so much more information from density estimation than simple summary statistics (e.g., the sample mean) in terms of explorable data analysis.  
 
 \item[(e)] (5 points) Again we will consider the variable \textsf{orientation}. We will estimate the conditional {\it joint} distribution of the volume of the \textsf{amygdala} and \textsf{acc}, conditioning on  a function of political \textsf{orientation}: $p(\textsf{amygdala}, \textsf{acc}|\textsf{orientation}=c)$, $c = 2, \ldots, 5$. You will use two-dimensional KDE to achieve the goal; et an appropriate kernel bandwidth $h >0$. Please show the two-dimensional KDE (e.g., two-dimensional heat-map, two-dimensional contour plot, etc.). 
 
 Please explain based on the results, can you infer that the conditional distribution of two variables (\textsf{amygdala}, \textsf{acc}) are different from $c = 2, \ldots, 5$? This is a type of scientific question one could infer from the data: Whether or not there is a difference between brain structure and political view.
 
  
 \end{enumerate}


\clearpage

\subsection*{6. To subtract or not to subtract, that is the question [Bonus 5 points].}

In PCA, we have to subtract the mean to form the covariance matrix 
\[
C = \frac 1 m \sum_{i=1}^m (x^i - \mu)(x^i - \mu)^T 
\]
before finding the weight vectors, where $\mu = \frac 1m \sum_{i=1}^m x^i$. For instance, we let
\[
 C w^1= \lambda_1 w^1
\]
where $\lambda_1$ is the largest eigenvalue of $C$, and $w^1$ is the corresponding largest eigenvector.  

Now suppose Prof. X insisting not subtracting the mean, and uses the eigenvectors of 
\[
\tilde C = \frac 1 m \sum_{i=1}^m x^i {x^i}^T
\]
to form the weight vectors. For instance, she lets $\tilde w^1$ to be such that 
\[
\tilde C \tilde w^1 = \tilde \lambda_1 \tilde w^1
\]
where $\tilde \lambda_1$ is the largest eigenvalue of $\tilde C$. 

Now the question is, are they the same (with and without subtract the mean)? Is $w^1$ equal or not equal to $\tilde w^1$? Use mathematical argument to justify your answer.


\end{document}
